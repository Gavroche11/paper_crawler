{
  "search_query": "\"Radiol Artif Intell\"[Journal] AND 2019:3000[pdat] AND (\"Journal Article\"[Publication Type] NOT \"Review\"[Publication Type] NOT \"Editorial\"[Publication Type] NOT \"Letter\"[Publication Type] NOT \"Comment\"[Publication Type])",
  "total_articles_found": 346,
  "keywords": [
    "language model",
    "llm",
    "gpt",
    "bert",
    "transformer",
    "nlp",
    "natural language processing",
    "chatgpt",
    "claude",
    "prompt",
    "prompting",
    "llama",
    "mistral",
    "gemini",
    "text-to-text",
    "text generation",
    "text embedding",
    "foundation model",
    "generative model"
  ],
  "language_model_articles_count": 30,
  "articles": [
    {
      "pmid": "39166972",
      "title": "Deep Learning-based Unsupervised Domain Adaptation via a Unified Model for Prostate Lesion Detection Using Multisite Biparametric MRI Datasets.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Sep",
      "doi": "10.1148/ryai.230521",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39166972/",
      "authors": [
        "Li H",
        "Liu H",
        "von Busch H",
        "Grimm R",
        "Huisman H",
        "Tong A",
        "Winkel D",
        "Penzkofer T",
        "Shabunin I",
        "Choi MH",
        "Yang Q",
        "Szolar D",
        "Shea S",
        "Coakley F",
        "Harisinghani M",
        "Oguz I",
        "Comaniciu D",
        "Kamen A",
        "Lou B"
      ],
      "abstract": "Purpose To determine whether the unsupervised domain adaptation (UDA) method with generated images improves the performance of a supervised learning (SL) model for prostate cancer (PCa) detection using multisite biparametric (bp) MRI datasets. Materials and Methods This retrospective study included data from 5150 patients (14\u2009191 samples) collected across nine different imaging centers. A novel UDA method using a unified generative model was developed for PCa detection using multisite bpMRI datasets. This method translates diffusion-weighted imaging (DWI) acquisitions, including apparent diffusion coefficient (ADC) and individual diffusion-weighted (DW) images acquired using various "
    },
    {
      "pmid": "38922031",
      "title": "Optimizing Performance of Transformer-based Models for Fetal Brain MR Image Segmentation.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Nov",
      "doi": "10.1148/ryai.230229",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38922031/",
      "authors": [
        "Pecco N",
        "Della Rosa PA",
        "Canini M",
        "Nocera G",
        "Scifo P",
        "Cavoretto PI",
        "Candiani M",
        "Falini A",
        "Castellano A",
        "Baldoli C"
      ],
      "abstract": "Purpose To test the performance of a transformer-based model when manipulating pretraining weights, dataset size, and input size and comparing the best model with the reference standard and state-of-the-art models for a resting-state functional (rs-fMRI) fetal brain extraction task. Materials and Methods An internal retrospective dataset (172 fetuses, 519 images; collected 2018-2022) was used to investigate influence of dataset size, pretraining approaches, and image input size on Swin-U-Net transformer (UNETR) and UNETR models. The internal and external (131 fetuses, 561 images) datasets were used to cross-validate and to assess generalization capability of the best model versus state-of-the-art models on different scanner types and number of gestational weeks (GWs). The Dice similarity coefficient (DSC) and the balanced average Hausdorff distance (BAHD) were used as segmentation performance metrics. Generalized equation estimation multifactorial models were used to assess significant model and interaction effects of interest. Results The Swin-UNETR model was not affected by the pretraining approach and dataset size and performed best with the mean dataset image size, with a mean DSC of 0.92 and BAHD of 0.097. Swin-UNETR was not affected by scanner type. Generalization results on the internal dataset showed that Swin-UNETR had lower performance compared with the reference standard models and comparable performance on the external dataset. Cross-validation on internal and external test sets demonstrated better and comparable performance of Swin-UNETR versus convolutional neural network architectures during the late-fetal period (GWs > 25) but lower performance during the midfetal period (GWs \u2264 25). Conclusion Swin-UNTER showed flexibility in dealing with smaller datasets, regardless of pretraining approaches. For fetal brain extraction from rs-fMR images, Swin-UNTER showed comparable performance with that of reference standard models during the late-fetal period and lower performance during the early GW period. "
    },
    {
      "pmid": "38717292",
      "title": "Performance of an Open-Source Large Language Model in Extracting Information from Free-Text Radiology Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Jul",
      "doi": "10.1148/ryai.230364",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38717292/",
      "authors": [
        "Le Guellec B",
        "Lef\u00e8vre A",
        "Geay C",
        "Shorten L",
        "Bruge C",
        "Hacein-Bey L",
        "Amouyel P",
        "Pruvo JP",
        "Kuchcinski G",
        "Hamroun A"
      ],
      "abstract": "Purpose To assess the performance of a local open-source large language model (LLM) in various information extraction tasks from real-life emergency brain MRI reports. Materials and Methods All consecutive emergency brain MRI reports written in 2022 from a French quaternary center were retrospectively reviewed. Two radiologists identified MRI scans that were performed in the emergency department for headaches. Four radiologists scored the reports' conclusions as either normal or abnormal. Abnormalities were labeled as either headache-causing or incidental. Vicuna (LMSYS Org), an open-source LLM, performed the same tasks. Vicuna's performance metrics were evaluated using the radiologists' consensus as the reference standard. Results Among the 2398 reports during the study period, radiologists identified 595 that included headaches in the indication (median age of patients, 35 years [IQR, 26-51 years]; 68% [403 of 595] women). A positive finding was reported in 227 of 595 (38%) cases, 136 of which could explain the headache. The LLM had a sensitivity of 98.0% (95% CI: 96.5, 99.0) and specificity of 99.3% (95% CI: 98.8, 99.7) for detecting the presence of headache in the clinical context, a sensitivity of 99.4% (95% CI: 98.3, 99.9) and specificity of 98.6% (95% CI: 92.2, 100.0) for the use of contrast medium injection, a sensitivity of 96.0% (95% CI: 92.5, 98.2) and specificity of 98.9% (95% CI: 97.2, 99.7) for study categorization as either normal or abnormal, and a sensitivity of 88.2% (95% CI: 81.6, 93.1) and specificity of 73% (95% CI: 62, 81) for causal inference between MRI findings and headache. Conclusion An open-source LLM was able to extract information from free-text radiology reports with excellent accuracy without requiring further training. "
    },
    {
      "pmid": "38294327",
      "title": "The LLM Will See You Now: Performance of ChatGPT on the Brazilian Radiology and Diagnostic Imaging and Mammography Board Examinations.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Jan",
      "doi": "10.1148/ryai.230568",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38294327/",
      "authors": [
        "Trivedi H",
        "Wawira Gichoya J"
      ],
      "abstract": ""
    },
    {
      "pmid": "38294325",
      "title": "Performance of ChatGPT on the Brazilian Radiology and Diagnostic Imaging and Mammography Board Examinations.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Jan",
      "doi": "10.1148/ryai.230103",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38294325/",
      "authors": [
        "Almeida LC",
        "Farina EMJM",
        "Kuriki PEA",
        "Abdala N",
        "Kitamura FC"
      ],
      "abstract": "This prospective exploratory study conducted from January 2023 through May 2023 evaluated the ability of ChatGPT to answer questions from Brazilian radiology board examinations, exploring how different prompt strategies can influence performance using GPT-3.5 and GPT-4. Three multiple-choice board examinations that did not include image-based questions were evaluated: "
    },
    {
      "pmid": "38265301",
      "title": "Generative Large Language Models for Detection of Speech Recognition Errors in Radiology Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Mar",
      "doi": "10.1148/ryai.230205",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38265301/",
      "authors": [
        "Schmidt RA",
        "Seah JCY",
        "Cao K",
        "Lim L",
        "Lim W",
        "Yeung J"
      ],
      "abstract": "This study evaluated the ability of generative large language models (LLMs) to detect speech recognition errors in radiology reports. A dataset of 3233 CT and MRI reports was assessed by radiologists for speech recognition errors. Errors were categorized as clinically significant or not clinically significant. Performances of five generative LLMs-GPT-3.5-turbo, GPT-4, text-davinci-003, Llama-v2-70B-chat, and Bard-were compared in detecting these errors, using manual error detection as the reference standard. Prompt engineering was used to optimize model performance. GPT-4 demonstrated high accuracy in detecting clinically significant errors (precision, 76.9%; recall, 100%; F1 score, 86.9%) and not clinically significant errors (precision, 93.9%; recall, 94.7%; F1 score, 94.3%). Text-davinci-003 achieved F1 scores of 72% and 46.6% for clinically significant and not clinically significant errors, respectively. GPT-3.5-turbo obtained 59.1% and 32.2% F1 scores, while Llama-v2-70B-chat scored 72.8% and 47.7%. Bard showed the lowest accuracy, with F1 scores of 47.5% and 20.9%. GPT-4 effectively identified challenging errors of nonsense phrases and internally inconsistent statements. Longer reports, resident dictation, and overnight shifts were associated with higher error rates. In conclusion, advanced generative LLMs show potential for automatic detection of speech recognition errors in radiology reports. "
    },
    {
      "pmid": "38197796",
      "title": "Vision Transformer-based Decision Support for Neurosurgical Intervention in Acute Traumatic Brain Injury: Automated Surgical Intervention Support Tool.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2024 Mar",
      "doi": "10.1148/ryai.230088",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38197796/",
      "authors": [
        "Smith CW",
        "Malhotra AK",
        "Hammill C",
        "Beaton D",
        "Harrington EM",
        "He Y",
        "Shakil H",
        "McFarlan A",
        "Jones B",
        "Lin HM",
        "Mathieu F",
        "Nathens AB",
        "Ackery AD",
        "Mok G",
        "Mamdani M",
        "Mathur S",
        "Wilson JR",
        "Moreland R",
        "Colak E",
        "Witiw CD"
      ],
      "abstract": "Purpose To develop an automated triage tool to predict neurosurgical intervention for patients with traumatic brain injury (TBI). Materials and Methods A provincial trauma registry was reviewed to retrospectively identify patients with TBI from 2005 to 2022 treated at a specialized Canadian trauma center. Model training, validation, and testing were performed using head CT scans with binary reference standard patient-level labels corresponding to whether the patient received neurosurgical intervention. Performance and accuracy of the model, the Automated Surgical Intervention Support Tool for TBI (ASIST-TBI), were also assessed using a held-out consecutive test set of all patients with TBI presenting to the center between March 2021 and September 2022. Results Head CT scans from 2806 patients with TBI (mean age, 57 years \u00b1 22 [SD]; 1955 [70%] men) were acquired between 2005 and 2021 and used for training, validation, and testing. Consecutive scans from an additional 612 patients (mean age, 61 years \u00b1 22; 443 [72%] men) were used to assess the performance of ASIST-TBI. There was accurate prediction of neurosurgical intervention with an area under the receiver operating characteristic curve (AUC) of 0.92 (95% CI: 0.88, 0.94), accuracy of 87% (491 of 562), sensitivity of 87% (196 of 225), and specificity of 88% (295 of 337) on the test dataset. Performance on the held-out test dataset remained robust with an AUC of 0.89 (95% CI: 0.85, 0.91), accuracy of 84% (517 of 612), sensitivity of 85% (199 of 235), and specificity of 84% (318 of 377). Conclusion A novel deep learning model was developed that could accurately predict the requirement for neurosurgical intervention using acute TBI CT scans. "
    },
    {
      "pmid": "38074793",
      "title": "Domain-adapted Large Language Models for Classifying Nuclear Medicine Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Nov",
      "doi": "10.1148/ryai.220281",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074793/",
      "authors": [
        "Huemann Z",
        "Lee C",
        "Hu J",
        "Cho SY",
        "Bradshaw TJ"
      ],
      "abstract": "PURPOSE: To evaluate the impact of domain adaptation on the performance of language models in predicting five-point Deauville scores on the basis of clinical fluorine 18 fluorodeoxyglucose PET/CT reports. MATERIALS AND METHODS: The authors retrospectively retrieved 4542 text reports and images for fluorodeoxyglucose PET/CT lymphoma examinations from 2008 to 2018 in the University of Wisconsin-Madison institutional clinical imaging database. Of these total reports, 1664 had Deauville scores that were extracted from the reports and served as training labels. The bidirectional encoder representations from transformers (BERT) model and initialized BERT models BioClinicalBERT, RadBERT, and RoBERTa were adapted to the nuclear medicine domain by pretraining using masked language modeling. These domain-adapted models were then compared with the non-domain-adapted versions on the task of five-point Deauville score prediction. The language models were compared against vision models, multimodal vision-language models, and a nuclear medicine physician, with sevenfold Monte Carlo cross-validation. Means and SDs for accuracy are reported, with  RESULTS: Domain adaptation improved the performance of all language models ( CONCLUSION: Domain adaptation improved the performance of large language models in predicting Deauville scores in PET/CT reports."
    },
    {
      "pmid": "38074789",
      "title": "Risk of Bias in Chest Radiography Deep Learning Foundation Models.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Nov",
      "doi": "10.1148/ryai.230060",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074789/",
      "authors": [
        "Glocker B",
        "Jones C",
        "Roschewitz M",
        "Winzeck S"
      ],
      "abstract": "PURPOSE: To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biologic sex and race. MATERIALS AND METHODS: This Health Insurance Portability and Accountability Act-compliant retrospective study used 127\u2009118 chest radiographs from 42\u2009884 patients (mean age, 63 years \u00b1 17 [SD]; 23\u2009623 male, 19\u2009261 female) from the CheXpert dataset that were collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov-Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups. RESULTS: Ten of 12 pairwise comparisons across biologic sex and race showed statistically significant differences in the studied foundation model, compared with four significant tests in the baseline model. Significant differences were found between male and female ( CONCLUSION: The studied chest radiography foundation model demonstrated racial and sex-related bias, which led to disparate performance across patient subgroups; thus, this model may be unsafe for clinical applications."
    },
    {
      "pmid": "38074787",
      "title": "Bias in Foundation Models: Primum Non Nocere or Caveat Emptor?",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Nov",
      "doi": "10.1148/ryai.230384",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074787/",
      "authors": [
        "Czum J",
        "Parr S"
      ],
      "abstract": ""
    },
    {
      "pmid": "38074780",
      "title": "Reconsidering Conclusions of Bias Assessment in Medical Imaging Foundation Models.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Nov",
      "doi": "10.1148/ryai.230432",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074780/",
      "authors": [
        "Chaudhari AS",
        "Bluethgen C",
        "Ouyang D"
      ],
      "abstract": ""
    },
    {
      "pmid": "37795140",
      "title": "The Subgroup Imperative: Chest Radiograph Classifier Generalization Gaps in Patient, Setting, and Pathology Subgroups.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Sep",
      "doi": "10.1148/ryai.220270",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37795140/",
      "authors": [
        "Ahluwalia M",
        "Abdalla M",
        "Sanayei J",
        "Seyyed-Kalantari L",
        "Hussain M",
        "Ali A",
        "Fine B"
      ],
      "abstract": "PURPOSE: To externally test four chest radiograph classifiers on a large, diverse, real-world dataset with robust subgroup analysis. MATERIALS AND METHODS: In this retrospective study, adult posteroanterior chest radiographs (January 2016-December 2020) and associated radiology reports from Trillium Health Partners in Ontario, Canada, were extracted and de-identified. An open-source natural language processing tool was locally validated and used to generate ground truth labels for the 197\u2009540-image dataset based on the associated radiology report. Four classifiers generated predictions on each chest radiograph. Performance was evaluated using accuracy, positive predictive value, negative predictive value, sensitivity, specificity, F1 score, and Matthews correlation coefficient for the overall dataset and for patient, setting, and pathology subgroups. RESULTS: Classifiers demonstrated 68%-77% accuracy, 64%-75% sensitivity, and 82%-94% specificity on the external testing dataset. Algorithms showed decreased sensitivity for solitary findings (43%-65%), patients younger than 40 years (27%-39%), and patients in the emergency department (38%-60%) and decreased specificity on normal chest radiographs with support devices (59%-85%). Differences in sex and ancestry represented movements along an algorithm's receiver operating characteristic curve. CONCLUSION: Performance of deep learning chest radiograph classifiers was subject to patient, setting, and pathology factors, demonstrating that subgroup analysis is necessary to inform implementation and monitor ongoing performance to ensure optimal quality, safety, and equity."
    },
    {
      "pmid": "37293349",
      "title": "AI Transformers for Radiation Dose Reduction in Serial Whole-Body PET Scans.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 May",
      "doi": "10.1148/ryai.220246",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37293349/",
      "authors": [
        "Wang YJ",
        "Qu L",
        "Sheybani ND",
        "Luo X",
        "Wang J",
        "Hawk KE",
        "Theruvath AJ",
        "Gatidis S",
        "Xiao X",
        "Pribnow A",
        "Rubin D",
        "Daldrup-Link HE"
      ],
      "abstract": "PURPOSE: To develop a deep learning approach that enables ultra-low-dose, 1% of the standard clinical dosage (3 MBq/kg), ultrafast whole-body PET reconstruction in cancer imaging. MATERIALS AND METHODS: In this Health Insurance Portability and Accountability Act-compliant study, serial fluorine 18-labeled fluorodeoxyglucose PET/MRI scans of pediatric patients with lymphoma were retrospectively collected from two cross-continental medical centers between July 2015 and March 2020. Global similarity between baseline and follow-up scans was used to develop Masked-LMCTrans, a longitudinal multimodality coattentional convolutional neural network (CNN) transformer that provides interaction and joint reasoning between serial PET/MRI scans from the same patient. Image quality of the reconstructed ultra-low-dose PET was evaluated in comparison with a simulated standard 1% PET image. The performance of Masked-LMCTrans was compared with that of CNNs with pure convolution operations (classic U-Net family), and the effect of different CNN encoders on feature representation was assessed. Statistical differences in the structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and visual information fidelity (VIF) were assessed by two-sample testing with the Wilcoxon signed rank  RESULTS: The study included 21 patients (mean age, 15 years \u00b1 7 [SD]; 12 female) in the primary cohort and 10 patients (mean age, 13 years \u00b1 4; six female) in the external test cohort. Masked-LMCTrans-reconstructed follow-up PET images demonstrated significantly less noise and more detailed structure compared with simulated 1% extremely ultra-low-dose PET images. SSIM, PSNR, and VIF were significantly higher for Masked-LMCTrans-reconstructed PET ( CONCLUSION: Masked-LMCTrans achieved high image quality reconstruction of 1% low-dose whole-body PET images."
    },
    {
      "pmid": "37293346",
      "title": "Transformer-based Deep Neural Network for Breast Cancer Classification on Digital Breast Tomosynthesis Images.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 May",
      "doi": "10.1148/ryai.220159",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37293346/",
      "authors": [
        "Lee W",
        "Lee H",
        "Lee H",
        "Park EK",
        "Nam H",
        "Kooi T"
      ],
      "abstract": "PURPOSE: To develop an efficient deep neural network model that incorporates context from neighboring image sections to detect breast cancer on digital breast tomosynthesis (DBT) images. MATERIALS AND METHODS: The authors adopted a transformer architecture that analyzes neighboring sections of the DBT stack. The proposed method was compared with two baselines: an architecture based on three-dimensional (3D) convolutions and a two-dimensional model that analyzes each section individually. The models were trained with 5174 four-view DBT studies, validated with 1000 four-view DBT studies, and tested on 655 four-view DBT studies, which were retrospectively collected from nine institutions in the United States through an external entity. Methods were compared using area under the receiver operating characteristic curve (AUC), sensitivity at a fixed specificity, and specificity at a fixed sensitivity. RESULTS: On the test set of 655 DBT studies, both 3D models showed higher classification performance than did the per-section baseline model. The proposed transformer-based model showed a significant increase in AUC (0.88 vs 0.91,  CONCLUSION: A transformer-based deep neural network using data from neighboring sections improved breast cancer classification performance compared with a per-section baseline model and was more efficient than a model using 3D convolutions."
    },
    {
      "pmid": "37035437",
      "title": "BERT-based Transfer Learning in Sentence-level Anatomic Classification of Free-Text Radiology Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Mar",
      "doi": "10.1148/ryai.220097",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37035437/",
      "authors": [
        "Nishigaki D",
        "Suzuki Y",
        "Wataya T",
        "Kita K",
        "Yamagata K",
        "Sato J",
        "Kido S",
        "Tomiyama N"
      ],
      "abstract": "PURPOSE: To assess whether transfer learning with a bidirectional encoder representations from transformers (BERT) model, pretrained on a clinical corpus, can perform sentence-level anatomic classification of free-text radiology reports, even for anatomic classes with few positive examples. MATERIALS AND METHODS: This retrospective study included radiology reports of patients who underwent whole-body PET/CT imaging from December 2005 to December 2020. Each sentence in these reports (6272 sentences) was labeled by two annotators according to body part (\"brain,\" \"head & neck,\" \"chest,\" \"abdomen,\" \"limbs,\" \"spine,\" or \"others\"). The BERT-based transfer learning approach was compared with two baseline machine learning approaches: bidirectional long short-term memory (BiLSTM) and the count-based method. Area under the precision-recall curve (AUPRC) and area under the receiver operating characteristic curve (AUC) were computed for each approach, and AUCs were compared using the DeLong test. RESULTS: The BERT-based approach achieved a macro-averaged AUPRC of 0.88 for classification, outperforming the baselines. AUC results for BERT were significantly higher than those of BiLSTM for all classes and those of the count-based method for the \"brain,\" \"chest,\" \"abdomen,\" and \"others\" classes ( CONCLUSION: The BERT-based transfer learning approach outperformed the BiLSTM and count-based approaches in sentence-level anatomic classification of free-text radiology reports, even for anatomic classes with few labeled training data."
    },
    {
      "pmid": "37035429",
      "title": "Attention-based Saliency Maps Improve Interpretability of Pneumothorax Classification.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2023 Mar",
      "doi": "10.1148/ryai.220187",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37035429/",
      "authors": [
        "Wollek A",
        "Graf R",
        "\u010ce\u010datka S",
        "Fink N",
        "Willem T",
        "Sabel BO",
        "Lasser T"
      ],
      "abstract": "PURPOSE: To investigate the chest radiograph classification performance of vision transformers (ViTs) and interpretability of attention-based saliency maps, using the example of pneumothorax classification. MATERIALS AND METHODS: In this retrospective study, ViTs were fine-tuned for lung disease classification using four public datasets: CheXpert, Chest X-Ray 14, MIMIC CXR, and VinBigData. Saliency maps were generated using transformer multimodal explainability and gradient-weighted class activation mapping (GradCAM). Classification performance was evaluated on the Chest X-Ray 14, VinBigData, and Society for Imaging Informatics in Medicine-American College of Radiology (SIIM-ACR) Pneumothorax Segmentation datasets using the area under the receiver operating characteristic curve (AUC) analysis and compared with convolutional neural networks (CNNs). The explainability methods were evaluated with positive and negative perturbation, sensitivity-n, effective heat ratio, intra-architecture repeatability, and interarchitecture reproducibility. In the user study, three radiologists classified 160 chest radiographs with and without saliency maps for pneumothorax and rated their usefulness. RESULTS: ViTs had comparable chest radiograph classification AUCs compared with state-of-the-art CNNs: 0.95 (95% CI: 0.94, 0.95) versus 0.83 (95%, CI 0.83, 0.84) on Chest X-Ray 14, 0.84 (95% CI: 0.77, 0.91) versus 0.83 (95% CI: 0.76, 0.90) on VinBigData, and 0.85 (95% CI: 0.85, 0.86) versus 0.87 (95% CI: 0.87, 0.88) on SIIM-ACR. Both saliency map methods unveiled a strong bias toward pneumothorax tubes in the models. Radiologists found 47% of the attention-based and 39% of the GradCAM saliency maps useful. The attention-based methods outperformed GradCAM on all metrics. CONCLUSION: ViTs performed similarly to CNNs in chest radiograph classification, and their attention-based saliency maps were more useful to radiologists and outperformed GradCAM."
    },
    {
      "pmid": "36523643",
      "title": "Patient-specific Hip Arthroplasty Dislocation Risk Calculator: An Explainable Multimodal Machine Learning-based Approach.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Nov",
      "doi": "10.1148/ryai.220067",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36523643/",
      "authors": [
        "Khosravi B",
        "Rouzrokh P",
        "Maradit Kremers H",
        "Larson DR",
        "Johnson QJ",
        "Faghani S",
        "Kremers WK",
        "Erickson BJ",
        "Sierra RJ",
        "Taunton MJ",
        "Wyles CC"
      ],
      "abstract": "PURPOSE: To develop a multimodal machine learning-based pipeline to predict patient-specific risk of dislocation following primary total hip arthroplasty (THA). MATERIALS AND METHODS: This study retrospectively evaluated 17\u2009073 patients who underwent primary THA between 1998 and 2018. A test set of 1718 patients was held out. A hybrid network of EfficientNet-B4 and Swin-B transformer was developed to classify patients according to 5-year dislocation outcomes from preoperative anteroposterior pelvic radiographs and clinical characteristics (demographics, comorbidities, and surgical characteristics). The most informative imaging features, extracted by the mentioned model, were selected and concatenated with clinical features. A collection of these features was then used to train a multimodal survival XGBoost model to predict the individualized hazard of dislocation within 5 years. C index was used to evaluate the multimodal survival model on the test set and compare it with another clinical-only model trained only on clinical data. Shapley additive explanation values were used for model explanation. RESULTS: The study sample had a median age of 65 years (IQR: 18 years; 52.1% [8889] women) with a 5-year dislocation incidence of 2%. On the holdout test set, the clinical-only model achieved a C index of 0.64 (95% CI: 0.60, 0.68). The addition of imaging features boosted multimodal model performance to a C index of 0.74 (95% CI: 0.69, 0.78;  CONCLUSION: Due to its discrimination ability and explainability, this risk calculator can be a potential powerful dislocation risk stratification and THA planning tool."
    },
    {
      "pmid": "36523642",
      "title": "Toward Foundational Deep Learning Models for Medical Imaging in the New Era of Transformer Networks.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Nov",
      "doi": "10.1148/ryai.210284",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36523642/",
      "authors": [
        "Willemink MJ",
        "Roth HR",
        "Sandfort V"
      ],
      "abstract": "Deep learning models are currently the cornerstone of artificial intelligence in medical imaging. While progress is still being made, the generic technological core of convolutional neural networks (CNNs) has had only modest innovations over the last several years, if at all. There is thus a need for improvement. More recently, transformer networks have emerged that replace convolutions with a complex attention mechanism, and they have already matched or exceeded the performance of CNNs in many tasks. Transformers need very large amounts of training data, even more than CNNs, but obtaining well-curated labeled data is expensive and difficult. A possible solution to this issue would be transfer learning with pretraining on a self-supervised task using very large amounts of unlabeled medical data. This pretrained network could then be fine-tuned on specific medical imaging tasks with relatively modest data requirements. The authors believe that the availability of a large-scale, three-dimension-capable, and extensively pretrained transformer model would be highly beneficial to the medical imaging and research community. In this article, authors discuss the challenges and obstacles of training a very large medical imaging transformer, including data needs, biases, training tasks, network architecture, privacy concerns, and computational requirements. The obstacles are substantial but not insurmountable for resourceful collaborative teams that may include academia and information technology industry partners. \u00a9 RSNA, 2022 "
    },
    {
      "pmid": "36523640",
      "title": "Visual Transformers and Convolutional Neural Networks for Disease Classification on Radiographs: A Comparison of Performance, Sample Efficiency, and Hidden Stratification.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Nov",
      "doi": "10.1148/ryai.220012",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36523640/",
      "authors": [
        "Murphy ZR",
        "Venkatesh K",
        "Sulam J",
        "Yi PH"
      ],
      "abstract": "PURPOSE: To compare performance, sample efficiency, and hidden stratification of visual transformer (ViT) and convolutional neural network (CNN) architectures for diagnosis of disease on chest radiographs and extremity radiographs using transfer learning. MATERIALS AND METHODS: In this HIPAA-compliant retrospective study, the authors fine-tuned data-efficient image transformers (DeiT) ViT and CNN classification models pretrained on ImageNet using the National Institutes of Health Chest X-ray 14 dataset (112\u2009120 images) and MURA dataset (14\u2009656 images) for thoracic disease and extremity abnormalities, respectively. Performance was assessed on internal test sets and 75\u2009000 external chest radiographs (three datasets). The primary comparison was DeiT-B ViT vs DenseNet121 CNN; secondary comparisons included DeiT-Ti (Tiny), ResNet152, and EfficientNetB7. Sample efficiency was evaluated by training models on varying dataset sizes. Hidden stratification was evaluated by comparing prevalence of chest tubes in pneumothorax false-positive and false-negative predictions and specific abnormalities for MURA false-negative predictions. RESULTS: DeiT-B weighted area under the receiver operating characteristic curve (wAUC) was slightly lower than that for DenseNet121 on chest radiograph (0.78 vs 0.79;  CONCLUSION: Although DeiT models had lower wAUCs than CNNs for chest radiograph and extremity domains, the differences may be negligible in clinical practice. DeiT-B had sample efficiency similar to that of DenseNet121 and may be less susceptible to certain types of hidden stratification."
    },
    {
      "pmid": "36204531",
      "title": "Deep Learning-based Assessment of Oncologic Outcomes from Natural Language Processing of Structured Radiology Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Sep",
      "doi": "10.1148/ryai.220055",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36204531/",
      "authors": [
        "Fink MA",
        "Kades K",
        "Bischoff A",
        "Moll M",
        "Schnell M",
        "K\u00fcchler M",
        "K\u00f6hler G",
        "Sellner J",
        "Heussel CP",
        "Kauczor HU",
        "Schlemmer HP",
        "Maier-Hein K",
        "Weber TF",
        "Kleesiek J"
      ],
      "abstract": "PURPOSE: To train a deep natural language processing (NLP) model, using data mined structured oncology reports (SOR), for rapid tumor response category (TRC) classification from free-text oncology reports (FTOR) and to compare its performance with human readers and conventional NLP algorithms. MATERIALS AND METHODS: In this retrospective study, databases of three independent radiology departments were queried for SOR and FTOR dated from March 2018 to August 2021. An automated data mining and curation pipeline was developed to extract Response Evaluation Criteria in Solid Tumors-related TRCs for SOR for ground truth definition. The deep NLP bidirectional encoder representations from transformers (BERT) model and three feature-rich algorithms were trained on SOR to predict TRCs in FTOR. Models' F1 scores were compared against scores of radiologists, medical students, and radiology technologist students. Lexical and semantic analyses were conducted to investigate human and model performance on FTOR. RESULTS: Oncologic findings and TRCs were accurately mined from 9653 of 12\u2009833 (75.2%) queried SOR, yielding oncology reports from 10\u2009455 patients (mean age, 60 years \u00b1 14 [SD]; 5303 women) who met inclusion criteria. On 802 FTOR in the test set, BERT achieved better TRC classification results (F1, 0.70; 95% CI: 0.68, 0.73) than the best-performing reference linear support vector classifier (F1, 0.63; 95% CI: 0.61, 0.66) and technologist students (F1, 0.65; 95% CI: 0.63, 0.67), had similar performance to medical students (F1, 0.73; 95% CI: 0.72, 0.75), but was inferior to radiologists (F1, 0.79; 95% CI: 0.78, 0.81). Lexical complexity and semantic ambiguities in FTOR influenced human and model performance, revealing maximum F1 score drops of -0.17 and -0.19, respectively. CONCLUSION: The developed deep NLP model reached the performance level of medical students but not radiologists in curating oncologic outcomes from radiology FTOR."
    },
    {
      "pmid": "35923380",
      "title": "Using BERT Models to Label Radiology Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Jul",
      "doi": "10.1148/ryai.220124",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35923380/",
      "authors": [
        "Zech JR"
      ],
      "abstract": ""
    },
    {
      "pmid": "35923379",
      "title": "On the Opportunities and Risks of Foundation Models for Natural Language Processing in Radiology.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Jul",
      "doi": "10.1148/ryai.220119",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35923379/",
      "authors": [
        "Wiggins WF",
        "Tejani AS"
      ],
      "abstract": ""
    },
    {
      "pmid": "35923377",
      "title": "Performance of Multiple Pretrained BERT Models to Automate and Accelerate Data Annotation for Large Datasets.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Jul",
      "doi": "10.1148/ryai.220007",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35923377/",
      "authors": [
        "Tejani AS",
        "Ng YS",
        "Xi Y",
        "Fielding JR",
        "Browning TG",
        "Rayan JC"
      ],
      "abstract": "PURPOSE: To develop and evaluate domain-specific and pretrained bidirectional encoder representations from transformers (BERT) models in a transfer learning task on varying training dataset sizes to annotate a larger overall dataset. MATERIALS AND METHODS: The authors retrospectively reviewed 69\u2009095 anonymized adult chest radiograph reports (reports dated April 2020-March 2021). From the overall cohort, 1004 reports were randomly selected and labeled for the presence or absence of each of the following devices: endotracheal tube (ETT), enterogastric tube (NGT, or Dobhoff tube), central venous catheter (CVC), and Swan-Ganz catheter (SGC). Pretrained transformer models (BERT, PubMedBERT, DistilBERT, RoBERTa, and DeBERTa) were trained, validated, and tested on 60%, 20%, and 20%, respectively, of these reports through fivefold cross-validation. Additional training involved varying dataset sizes with 5%, 10%, 15%, 20%, and 40% of the 1004 reports. The best-performing epochs were used to assess area under the receiver operating characteristic curve (AUC) and determine run time on the overall dataset. RESULTS: The highest average AUCs from fivefold cross-validation were 0.996 for ETT (RoBERTa), 0.994 for NGT (RoBERTa), 0.991 for CVC (PubMedBERT), and 0.98 for SGC (PubMedBERT). DeBERTa demonstrated the highest AUC for each support device trained on 5% of the training set. PubMedBERT showed a higher AUC with a decreasing training set size compared with BERT. Training and validation time was shortest for DistilBERT at 3 minutes 39 seconds on the annotated cohort. CONCLUSION: Pretrained and domain-specific transformer models required small training datasets and short training times to create a highly accurate final model that expedites autonomous annotation of large datasets."
    },
    {
      "pmid": "35923376",
      "title": "RadBERT: Adapting Transformer-based Language Models to Radiology.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Jul",
      "doi": "10.1148/ryai.210258",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35923376/",
      "authors": [
        "Yan A",
        "McAuley J",
        "Lu X",
        "Du J",
        "Chang EY",
        "Gentili A",
        "Hsu CN"
      ],
      "abstract": "PURPOSE: To investigate if tailoring a transformer-based language model to radiology is beneficial for radiology natural language processing (NLP) applications. MATERIALS AND METHODS: This retrospective study presents a family of bidirectional encoder representations from transformers (BERT)-based language models adapted for radiology, named RadBERT. Transformers were pretrained with either 2.16 or 4.42 million radiology reports from U.S. Department of Veterans Affairs health care systems nationwide on top of four different initializations (BERT-base, Clinical-BERT, robustly optimized BERT pretraining approach [RoBERTa], and BioMed-RoBERTa) to create six variants of RadBERT. Each variant was fine-tuned for three representative NLP tasks in radiology:  RESULTS: For abnormal sentence classification, all models performed well (accuracies above 97.5 and F1 scores above 95.0). RadBERT variants achieved significantly higher scores than corresponding baselines when given only 10% or less of 12\u2009458 annotated training sentences. For report coding, all variants outperformed baselines significantly for all five coding systems. The variant RadBERT-BioMed-RoBERTa performed the best among all models for report summarization, achieving a Recall-Oriented Understudy for Gisting Evaluation-1 score of 16.18 compared with 15.27 by the corresponding baseline (BioMed-RoBERTa,  CONCLUSION: Transformer-based language models tailored to radiology had improved performance of radiology NLP tasks compared with baseline transformer language models."
    },
    {
      "pmid": "35923373",
      "title": "Application of a Domain-specific BERT for Detection of Speech Recognition Errors in Radiology Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Jul",
      "doi": "10.1148/ryai.210185",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35923373/",
      "authors": [
        "Chaudhari GR",
        "Liu T",
        "Chen TL",
        "Joseph GB",
        "Vella M",
        "Lee YJ",
        "Vu TH",
        "Seo Y",
        "Rauschecker AM",
        "McCulloch CE",
        "Sohn JH"
      ],
      "abstract": "PURPOSE: To develop radiology domain-specific bidirectional encoder representations from transformers (BERT) models that can identify speech recognition (SR) errors and suggest corrections in radiology reports. MATERIALS AND METHODS: A pretrained BERT model, Clinical BioBERT, was further pretrained on a corpus of 114\u2009008 radiology reports between April 2016 and August 2019 that were retrospectively collected from two hospitals. Next, the model was fine-tuned on a training dataset of generated insertion, deletion, and substitution errors, creating Radiology BERT. This model was retrospectively evaluated on an independent dataset of radiology reports with generated errors ( RESULTS: Radiology-specific BERT had AUC values of >.99 (95% CI: >0.99, >0.99), 0.94 (95% CI: 0.93, 0.94), 0.98 (95% CI: 0.98, 0.98), and 0.97 (95% CI: 0.97, 0.97) for detecting insertion, deletion, substitution, and all errors, respectively, on the independently generated test set. Testing on unaltered report impressions revealed a sensitivity of 82% (28 of 34; 95% CI: 70%, 93%) and specificity of 88% (1521 of 1728; 95% CI: 87%, 90%). Testing on prospective SR errors showed an accuracy of 75% (69 of 92; 95% CI: 65%, 83%). Finally, the correct word was the top suggestion for 45.6% (475 of 1041; 95% CI: 42.5%, 49.3%) of errors. CONCLUSION: Radiology-specific BERT models fine-tuned on generated errors were able to identify SR errors in radiology reports and suggest corrections."
    },
    {
      "pmid": "35652113",
      "title": "Deep Learning for the Detection, Localization, and Characterization of Focal Liver Lesions on Abdominal US Images.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 May",
      "doi": "10.1148/ryai.210110",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35652113/",
      "authors": [
        "Dadoun H",
        "Rousseau AL",
        "de Kerviler E",
        "Correas JM",
        "Tissier AM",
        "Joujou F",
        "Bodard S",
        "Khezzane K",
        "de Margerie-Mellon C",
        "Delingette H",
        "Ayache N"
      ],
      "abstract": "PURPOSE: To train and assess the performance of a deep learning-based network designed to detect, localize, and characterize focal liver lesions (FLLs) in the liver parenchyma on abdominal US images. MATERIALS AND METHODS: In this retrospective, multicenter, institutional review board-approved study, two object detectors, Faster region-based convolutional neural network (Faster R-CNN) and Detection Transformer (DETR), were fine-tuned on a dataset of 1026 patients ( RESULTS: DETR achieved a specificity of 90% (95% CI: 75, 100) and a sensitivity of 97% (95% CI: 97, 97) for the detection of FLLs. The performance of DETR met or exceeded that of the three caregivers for this task. DETR correctly localized 80% of the lesions, and it achieved a specificity of 81% (95% CI: 67, 91) and a sensitivity of 82% (95% CI: 62, 100) for FLL characterization (benign vs malignant) among lesions localized by all raters. The performance of DETR met or exceeded that of two experts and Faster R-CNN for these tasks. CONCLUSION: DETR demonstrated high specificity for detection, localization, and characterization of FLLs on abdominal US images. "
    },
    {
      "pmid": "35391762",
      "title": "Automated Identification and Measurement Extraction of Pancreatic Cystic Lesions from Free-Text Radiology Reports Using Natural Language Processing.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Mar",
      "doi": "10.1148/ryai.210092",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35391762/",
      "authors": [
        "Yamashita R",
        "Bird K",
        "Cheung PY",
        "Decker JH",
        "Flory MN",
        "Goff D",
        "Morimoto LN",
        "Shon A",
        "Wentland AL",
        "Rubin DL",
        "Desser TS"
      ],
      "abstract": "PURPOSE: To automatically identify a cohort of patients with pancreatic cystic lesions (PCLs) and extract PCL measurements from historical CT and MRI reports using natural language processing (NLP) and a question answering system. MATERIALS AND METHODS: Institutional review board approval was obtained for this retrospective Health Insurance Portability and Accountability Act-compliant study, and the requirement to obtain informed consent was waived. A cohort of free-text CT and MRI reports generated between January 1991 and July 2019 that covered the pancreatic region were identified. A PCL identification model was developed by modifying a rule-based information extraction model; measurement extraction was performed using a state-of-the-art question answering system. The system's performance was evaluated against radiologists' annotations. RESULTS: For this study, 430\u2009426 free-text radiology reports from 199\u2009783 unique patients were identified. The NLP model for identifying PCL was applied to 1000 test samples. The interobserver agreement between the model and two radiologists was almost perfect (Fleiss \u03ba = 0.951), and the false-positive rate and true-positive rate were 3.0% and 98.2%, respectively, against consensus of radiologists' annotations as ground truths. The overall accuracy and Lin concordance correlation coefficient for measurement extraction were 0.958 and 0.874, respectively, against radiologists' annotations as ground truths. CONCLUSION: An NLP-based system was developed that identifies patients with PCLs and extracts measurements from a large single-institution archive of free-text radiology reports. This approach may prove valuable to study the natural history and potential risks of PCLs and can be applied to many other use cases."
    },
    {
      "pmid": "35146435",
      "title": "Automatic Diagnosis Labeling of Cardiovascular MRI by Using Semisupervised Natural Language Processing of Text Reports.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2022 Jan",
      "doi": "10.1148/ryai.210085",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35146435/",
      "authors": [
        "Zaman S",
        "Petri C",
        "Vimalesvaran K",
        "Howard J",
        "Bharath A",
        "Francis D",
        "Peters N",
        "Cole GD",
        "Linton N"
      ],
      "abstract": "PURPOSE: To assess whether the semisupervised natural language processing (NLP) of text from clinical radiology reports could provide useful automated diagnosis categorization for ground truth labeling to overcome manual labeling bottlenecks in the machine learning pipeline. MATERIALS AND METHODS: In this retrospective study, 1503 text cardiac MRI reports from 2016 to 2019 were manually annotated for five diagnoses by clinicians: normal, dilated cardiomyopathy (DCM), hypertrophic cardiomyopathy, myocardial infarction (MI), and myocarditis. A semisupervised method that uses bidirectional encoder representations from transformers (BERT) pretrained on 1.14 million scientific publications was fine-tuned by using the manually extracted labels, with a report dataset split into groups of 801 for training, 302 for validation, and 400 for testing. The model's performance was compared with two traditional NLP models: a rule-based model and a support vector machine (SVM) model. The models' F1 scores and receiver operating characteristic curves were used to analyze performance. RESULTS: After 15 epochs, the F1 scores on the test set of 400 reports were as follows: normal, 84%; DCM, 79%; hypertrophic cardiomyopathy, 86%; MI, 91%; and myocarditis, 86%. The pooled F1 score and area under the receiver operating curve were 86% and 0.96, respectively. On the same test set, the BERT model had a higher performance than the rule-based model (F1 score, 42%) and SVM model (F1 score, 82%). Diagnosis categories classified by using the BERT model performed the labeling of 1000 MR images in 0.2 second. CONCLUSION: The developed model used labels extracted from radiology reports to provide automated diagnosis categorization of MR images with a high level of performance."
    },
    {
      "pmid": "34350414",
      "title": "Natural Language Processing of Radiology Text Reports: Interactive Text Classification.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2021 Jul",
      "doi": "10.1148/ryai.2021210035",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34350414/",
      "authors": [
        "Wiggins WF",
        "Kitamura F",
        "Santos I",
        "Prevedello LM"
      ],
      "abstract": "This report presents a hands-on introduction to natural language processing (NLP) of radiology reports with deep neural networks in Google Colaboratory (Colab) to introduce readers to the rapidly evolving field of NLP. The implementation of the Google Colab notebook was designed with code hidden to facilitate learning for noncoders (ie, individuals with little or no computer programming experience). The data used for this module are the corpus of radiology reports from the Indiana University chest x-ray collection available from the National Library of Medicine's Open-I service. The module guides learners through the process of exploring the data, splitting the data for model training and testing, preparing the data for NLP analysis, and training a deep NLP model to classify the reports as normal or abnormal. Concepts in NLP, such as tokenization, numericalization, language modeling, and word embeddings, are demonstrated in the module. The module is implemented in a guided fashion with the authors presenting the material and explaining concepts. Interactive features and extensive text commentary are provided directly in the notebook to facilitate self-guided learning and experimentation with the module. "
    },
    {
      "pmid": "33937782",
      "title": "Combination of Active Transfer Learning and Natural Language Processing to Improve Liver Volumetry Using Surrogate Metrics with Deep Learning.",
      "journal": "Radiology. Artificial intelligence",
      "pub_date": "2019 Jan",
      "doi": "10.1148/ryai.2019180019",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33937782/",
      "authors": [
        "Marinelli B",
        "Kang M",
        "Martini M",
        "Zech JR",
        "Titano J",
        "Cho S",
        "Costa AB",
        "Oermann EK"
      ],
      "abstract": "PURPOSE: To determine if weakly supervised learning with surrogate metrics and active transfer learning can hasten clinical deployment of deep learning models. MATERIALS AND METHODS: By leveraging Liver Tumor Segmentation (LiTS) challenge 2017 public data ( RESULTS: Data from patients with poor liver volume prediction ( CONCLUSION: Active transfer learning using surrogate metrics facilitated deployment of deep learning models for clinically meaningful liver segmentation at a major liver transplant center.\u00a9 RSNA, 2019"
    }
  ]
}